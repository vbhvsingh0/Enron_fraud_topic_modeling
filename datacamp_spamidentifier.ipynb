{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6673bd3f-06c8-4b5f-932f-bcd334fa57ba",
   "metadata": {},
   "source": [
    "In this project you're going to work with text data, containing emails from Enron employees. The Enron scandal is a famous fraud case. Enron employees covered up the bad financial position of the company, thereby keeping the stock price artificially high. Enron employees sold their own stock options, and when the truth came out, Enron investors were left with nothing. The goal is to find all emails that mention specific words, such as \"sell enron stock\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10374f0b-0292-422a-9b91-773b72455b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f532442e-297a-4a5b-8a8a-6d36791b6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('assets_prctce/enron_emails_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a9ab9e-87ba-47c6-9326-b8543c2ae140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all cleaned emails that contain 'sell enron stock'\n",
    "mask = df['clean_content'].str.contains('sell enron stock', na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "616fb538-1258-4811-b54a-77de910ea9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Message-ID                        From  \\\n",
      "154  <6336501.1075841154311.JavaMail.evans@thyme>  ('sarah.palmer@enron.com')   \n",
      "\n",
      "                             To                 Date  \\\n",
      "154  ('sarah.palmer@enron.com')  2002-02-01 14:53:35   \n",
      "\n",
      "                                               content  \\\n",
      "154  \\nJoint Venture: A 1997 Enron Meeting Belies O...   \n",
      "\n",
      "                                         clean_content  \n",
      "154  joint venture enron meeting belies officers cl...  \n"
     ]
    }
   ],
   "source": [
    "print(df.loc[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ed0a5f0-3d40-40a8-b2b3-470842768134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Message-ID  \\\n",
      "0      <8345058.1075840404046.JavaMail.evans@thyme>   \n",
      "1      <1512159.1075863666797.JavaMail.evans@thyme>   \n",
      "2     <26118676.1075862176383.JavaMail.evans@thyme>   \n",
      "3     <10369289.1075860831062.JavaMail.evans@thyme>   \n",
      "4     <26728895.1075860815046.JavaMail.evans@thyme>   \n",
      "...                                             ...   \n",
      "1151  <15875618.1075860830584.JavaMail.evans@thyme>   \n",
      "1450  <30798399.1075841348382.JavaMail.evans@thyme>   \n",
      "1473    <957052.1075861359136.JavaMail.evans@thyme>   \n",
      "1557  <18936682.1075861158419.JavaMail.evans@thyme>   \n",
      "1621   <5472336.1075841501893.JavaMail.evans@thyme>   \n",
      "\n",
      "                                   From                                 To  \\\n",
      "0       ('advdfeedback@investools.com')    ('advdfeedback@investools.com')   \n",
      "1         ('richard.sanders@enron.com')      ('richard.sanders@enron.com')   \n",
      "2                 ('m..love@enron.com')              ('m..love@enron.com')   \n",
      "3          ('leslie.milosevich@kp.org')       ('leslie.milosevich@kp.org')   \n",
      "4          ('rtwait@graphicaljazz.com')       ('rtwait@graphicaljazz.com')   \n",
      "...                                 ...                                ...   \n",
      "1151             ('bandersn@loyno.edu')             ('bandersn@loyno.edu')   \n",
      "1450       ('chairman.enron@enron.com')       ('chairman.enron@enron.com')   \n",
      "1473         ('chairman.ken@enron.com')         ('chairman.ken@enron.com')   \n",
      "1557      ('resources.human@enron.com')      ('resources.human@enron.com')   \n",
      "1621  ('announcements.enron@enron.com')  ('announcements.enron@enron.com')   \n",
      "\n",
      "                     Date                                            content  \\\n",
      "0     2002-01-29 23:20:55  INVESTools Advisory\\nA Free Digest of Trusted ...   \n",
      "1     2000-09-20 19:07:00  ----- Forwarded by Richard B Sanders/HOU/ECT o...   \n",
      "2     2001-10-30 16:15:17  hey you are not wearing your target purple shi...   \n",
      "3     2002-01-30 17:54:18  Leslie Milosevich\\n1042 Santa Clara Avenue\\nAl...   \n",
      "4     2002-01-30 19:36:01  Rini Twait\\n1010 E 5th Ave\\nLongmont, CO 80501...   \n",
      "...                   ...                                                ...   \n",
      "1151  2002-01-30 17:54:12  Blanca Anderson\\n1310 Cadiz\\nNew orleans, LA 7...   \n",
      "1450  2002-01-16 14:45:55  \\nEnron announced today that its common stock ...   \n",
      "1473  2001-11-09 23:48:54  \\nToday, we announced plans to merge with Dyne...   \n",
      "1557  2001-11-25 23:15:46  We've updated the Merger Q&A document on our E...   \n",
      "1621  2002-01-23 20:51:34  \\nPLEASE READ THIS IMPORTANT INFORMATION CONCE...   \n",
      "\n",
      "                                          clean_content  \n",
      "0     investools advisory free digest trusted invest...  \n",
      "1     forwarded richard b sanders hou ect pm justin ...  \n",
      "2     hey wearing target purple shirt today mine wan...  \n",
      "3     leslie milosevich santa clara avenue alameda c...  \n",
      "4     rini twait e th ave longmont co rtwait graphic...  \n",
      "...                                                 ...  \n",
      "1151  blanca anderson cadiz new orleans la bandersn ...  \n",
      "1450  enron announced today common stock traded coun...  \n",
      "1473  today announced plans merge dynegy major playe...  \n",
      "1557  updated merger q document enron updates site h...  \n",
      "1621  please read important information concerning e...  \n",
      "\n",
      "[314 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of terms to search for\n",
    "searchfor = ['enron stock', 'sell stock', 'stock bonus', 'sell enron stock']\n",
    "\n",
    "# Filter cleaned emails on searchfor list and select from df \n",
    "filtered_emails = df.loc[df['clean_content'].str.contains('|'.join(searchfor), na=False)]\n",
    "print(filtered_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "630decff-bd87-4745-a4d6-7f0e2511d72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1776\n",
      "1     314\n",
      "Name: flag, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create flag variable where the emails match the searchfor terms\n",
    "df['flag'] = np.where((df['clean_content'].str.contains('|'.join(searchfor)) == True), 1, 0)\n",
    "\n",
    "# Count the values of the flag variable\n",
    "count = df['flag'].value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a8e1f14-5640-494a-a882-f2be954f9bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import nltk packages and string \n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Define stopwords to exclude\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\", \"ect\", \"u\", \"fwd\", \"www\", \"com\"))\n",
    "\n",
    "# Define punctuations to exclude and lemmatizer\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "799a4120-2266-4a23-9ba5-488fd77dbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the lemmatizer from nltk\n",
    "import nltk\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Define word cleaning function\n",
    "def clean(text, stop):\n",
    "    text = text.rstrip()\n",
    "\t# Remove stopwords\n",
    "    stop_free = \" \".join([word for word in text.lower().split() if ((word not in stop) and (not word.isdigit()))])\n",
    "\t# Remove punctuations\n",
    "    punc_free = ''.join(word for word in stop_free if word not in exclude)\n",
    "\t# Lemmatize all words\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())      \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8690a45-27bd-4d86-992e-9369f37becfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean the emails in df and print results\n",
    "#import nltk\n",
    "#nltk.download('omw-1.4')\n",
    "#nltk.download('wordnet')\n",
    "text_clean=[]\n",
    "for text in df['clean_content']:\n",
    "    text_clean.append(clean(str(text), stop).split())    \n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddae6395-bf0f-4436-b812-bac4b85da2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Define the dictionary\n",
    "dictionary = corpora.Dictionary(text_clean)\n",
    "\n",
    "# Define the corpus \n",
    "corpus = [dictionary.doc2bow(text) for text in text_clean]\n",
    "\n",
    "# Print corpus and dictionary\n",
    "#print(dictionary)\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be0ce202-751d-420d-9ebf-e691748f3780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.036*\"image\" + 0.034*\"td\" + 0.025*\"net\" + 0.023*\"money\" + 0.022*\"tr\"')\n",
      "(1, '0.014*\"message\" + 0.012*\"original\" + 0.012*\"enron\" + 0.010*\"e\" + 0.009*\"mail\"')\n",
      "(2, '0.022*\"enron\" + 0.013*\"employee\" + 0.011*\"outage\" + 0.011*\"pm\" + 0.010*\"schedule\"')\n",
      "(3, '0.007*\"bakernet\" + 0.006*\"market\" + 0.006*\"time\" + 0.006*\"service\" + 0.005*\"enron\"')\n",
      "(4, '0.041*\"enron\" + 0.008*\"hou\" + 0.007*\"company\" + 0.006*\"development\" + 0.005*\"corp\"')\n"
     ]
    }
   ],
   "source": [
    "# Define the LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=5)\n",
    "\n",
    "# Save the topics and top 5 words\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "\n",
    "# Print the results\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84773d27-3389-40ef-b48a-50cce8a3abc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
